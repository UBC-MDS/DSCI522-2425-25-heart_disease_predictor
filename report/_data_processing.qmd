## Data Cleaning Process

In order to prepare the raw heart disease dataset for analysis, 
several cleaning steps were undertaken to 
ensure the data is consistent, complete, and properly formatted. 
These steps include renaming columns, relabeling categorical values, 
and handling missing data. 

Below is a detailed description of each cleaning task performed:

### Cleaning Data
1. **Renaming Columns**:
   The original dataset contained column names that were unclear. 
   To make the dataset more understandable and easier to work with, the column names were 
   renamed to more descriptive, readable terms. 

2. **Relabeling Categorical Variables**:
   Several columns contained categorical data that needed to be relabeled for better interpretability:
   
| **Column Name**              | **Original Values**           | **New Values**                                                |
|------------------------------|-------------------------------|--------------------------------------------------------------|
| **Chest Pain Type**           | 1, 2, 3, 4                    | 'Typical angina', 'Atypical angina', 'Non-anginal pain', 'Asymptomatic' |
| **Fasting Blood Sugar**       | 'yes', 'no'                   | 1 (yes), 0 (no)                                              |
| **Resting ECG**               | 0, 1, 2                       | 'Normal', 'ST-T wave abnormality', 'Left ventricular hypertrophy' |
| **Exercise Induced Angina**   | 0, 1                          | 'No', 'Yes'                                                  |
| **Slope**                     | 1, 2, 3                       | 'Upsloping', 'Flat', 'Downsloping'                           |
| **Thalassemia**               | 3, 6, 7                       | 'Normal', 'Fixed defect', 'Reversible defect'                |
| **Diagnosis**                 | 2, 3, 4                       | 1 (presence of heart disease)                                | 
: Raw Table {#tbl-relabledTable}

   
3. **Handling Missing Values**:
   The raw dataset contained missing values that could interfere with analysis. 
   To address this, all rows with missing values were removed from the dataset. 
   
### Validating Data
   After cleaning, several validation checks were performed to ensure data quality:

   - **Empty Observations**: A check was run to ensure there were no rows where all values were missing.
   - **Missingness Threshold**: Missing values in numeric and categorical columns were validated to 
   ensure they did not exceed the defined thresholds (e.g., no more than 5% missing data in certain columns).
   - **Column Names and Data Types**: The dataset was validated for correct column names and data types 
   (e.g., integer, string, float) to ensure consistency with expectations.
   - **Duplicate Records**: A check for duplicate rows was conducted to ensure that no redundant 
   observations were present.
   - **Outliers and Anomalous Values**: Numerical columns were validated to ensure values fell 
   within acceptable ranges (e.g., age between 0 and 120, cholesterol between 100 and 600).
   - **Correct Categorical Levels**: Categorical variables were checked to ensure they only contained 
   valid values (e.g., 'typical angina', 'normal', etc.).
   - **Class Proportions**: The distribution of the target variable, "diagnosis," was checked to 
   ensure balanced proportions of 0 and 1, within a specified tolerance.
   - **Feature-Label Correlation**: A deep check was performed to ensure that the correlation between 
   features and the target variable ("diagnosis") did not exceed an acceptable threshold (0.9), 
   which could indicate potential data leakage or multicollinearity.
   - **Feature-Feature Correlation**: A check was conducted to ensure no pairs of features exhibited 
   highly correlated relationships that could lead to multicollinearity.

## Cleaned Dataset
   This dataset will be used onwards for all analysis.

   | Variable Name            | Role     | Type        | Description                                        | Units          | Missing Values |
   |--------------------------|----------|-------------|----------------------------------------------------|----------------|----------------|
   | age                      | Feature  | Integer     | Age of the patient in years                        | years          | no             |
   | sex                      | Feature  | Categorical | Male/Female                                        |                | no             |
   | chest_pain_type          | Feature  | Categorical | Chest pain type: [typical angina, atypical angina, non-anginal, asymptomatic] |                | no             |
   | resting_blood_pressure   | Feature  | Integer     | Resting blood pressure (in mm Hg on admission to the hospital) | mm Hg          | no             |
   | cholesterol              | Feature  | Integer     | Serum cholesterol in mg/dl                         | mg/dl          | no             |
   | fasting_blood_sugar      | Feature  | Categorical | If fasting blood sugar > 120 mg/dl                 |                | no             |
   | rest_ecg                 | Feature  | Categorical | Resting electrocardiographic results: [normal, stt abnormality, lv hypertrophy] |                | no             |
   | max_heart_rate           | Feature  | Integer     | Maximum heart rate achieved                        | bpm            | no             |
   | exercise_induced_angina  | Feature  | Categorical | Exercise-induced angina (True/False)               |                | no             |
   | st_depression            | Feature  | Integer     | ST depression induced by exercise relative to rest |                | no             |
   | slope                    | Feature  | Categorical | The slope of the peak exercise ST segment          |                | no             |
   | num_of_vessels           | Feature  | Integer     | Number of major vessels (0-3) colored by fluoroscopy|                | no             |
   | thalassemia              | Feature  | Categorical | Thalassemia: [normal, fixed defect, reversible defect] |                | no             |
   | diagnosis                | Target   | Categorical | Predicted attribute (1 = presence of heart disease, 0 = absence) |                | no             |

   : Cleaned Table {#tbl-cleanedTable}


<!-- TODO: Delete Below -->

```{python}
import numpy as np
import pandas as pd
import warnings
import pandera as pa
from pandera import Column, Check, DataFrameSchema


from deepchecks.tabular import Dataset
from deepchecks.tabular.checks import FeatureLabelCorrelation, FeatureFeatureCorrelation, PredictionDrift

warnings.filterwarnings('ignore')
```

```{python}
# Retrieve the data from the ucimlrepo
from ucimlrepo import fetch_ucirepo 

heart_disease = fetch_ucirepo(id=45)

X = heart_disease.data.features  
y = heart_disease.data.targets   

data = X.copy()
data['Diagnosis'] = y  

# print("Dataset Shape:", data.shape)
# print("Columns:", data.columns)

# Save the raw 

data.to_csv("../data/raw/raw_heart_disease_data.csv", index=False)
```

```{python}
df = pd.read_csv("../data/raw/raw_heart_disease_data.csv")
# df
```

```{python}
# Rename column names to be human readable

new_column_names = ["age", "sex", "chest_pain_type", "resting_blood_pressure", "cholesterol", "fasting_blood_sugar", 
                "rest_ecg", "max_heart_rate", "exercise_induced_angina", "st_depression", "slope", 
               "num_of_vessels", "thalassemia", "diagnosis"]


df.columns = new_column_names
```

<!-- ## Checking the unique values for each column -->

```{python}
# df['num_of_vessels'].unique()
```

```{python}
# df['sex'].unique()
```

```{python}
# df['chest_pain_type'].unique()
```

```{python}
# df['fasting_blood_sugar'].unique()
```

```{python}
# df['rest_ecg'].unique()
```

```{python}
# df['thalassemia'].unique()
```

<!-- ## Map the values with the provided labels -->

```{python}
df.loc[(df['chest_pain_type'] == 1), 'chest_pain_type'] = 'typical angina'
df.loc[(df['chest_pain_type'] == 2), 'chest_pain_type'] = 'atypical angina'
df.loc[(df['chest_pain_type'] == 3), 'chest_pain_type'] = 'non-anginal pain'
df.loc[(df['chest_pain_type'] == 4), 'chest_pain_type'] = 'asymptomatic'
```

```{python}
df.loc[(df['fasting_blood_sugar'] == 'yes'), 'fasting_blood_sugar'] = 1
df.loc[(df['fasting_blood_sugar'] == 'no'), 'fasting_blood_sugar'] = 0
```

```{python}
df.loc[(df['rest_ecg'] == 0), 'rest_ecg'] = 'normal'
df.loc[(df['rest_ecg'] == 1), 'rest_ecg'] = 'ST-T wave abnormality'
df.loc[(df['rest_ecg'] == 2), 'rest_ecg'] = 'left ventricular hypertrophy'
```

```{python}
df.loc[(df['exercise_induced_angina'] == 0), 'exercise_induced_angina'] = 'no'
df.loc[(df['exercise_induced_angina'] == 1), 'exercise_induced_angina'] = 'yes'
```

```{python}
df.loc[(df['slope'] == 1), 'slope'] = 'upsloping'
df.loc[(df['slope'] == 2), 'slope'] = 'flat'
df.loc[(df['slope'] == 3), 'slope'] = 'downsloping'
```

```{python}
df.loc[(df['thalassemia'] == 3.), 'thalassemia'] = 'normal'
df.loc[(df['thalassemia'] == 6.), 'thalassemia'] = 'fixed defect'
df.loc[(df['thalassemia'] == 7.), 'thalassemia'] = 'reversable defect'
```

```{python}
df.loc[(df['diagnosis'] == 2), 'diagnosis'] = 1
df.loc[(df['diagnosis'] == 3), 'diagnosis'] = 1
df.loc[(df['diagnosis'] == 4), 'diagnosis'] = 1
```

```{python}
# df
```

<!-- ## Checking and removing null values -->

```{python}
# df.isnull().sum()
```

```{python}
df = df.dropna()
```

```{python}
# df.isnull().sum()
```

## Data Validation

Since we have imported data from the ucimlrepo, we will not be checking for correct data file format. 

```{python}
# 3. Check for empty observations

# Build Schema
empty_obs_schema = pa.DataFrameSchema(
    checks=[
        Check(lambda df: ~(df.isna().all(axis=1)).any(), error="Empty rows found.")
    ]
)

# Validate Schema
try:
    empty_obs_schema.validate(df)
    # print("Validation passed: No empty observations found.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 4. Missingness not beyond expected threshold

# Build Schema
missingness_threshold_schema = pa.DataFrameSchema(
    {
        # a. Numeric columns with no missing values allowed
        "age": pa.Column(int, pa.Check.between(0, 120), nullable=False),
        "sex": pa.Column(int, pa.Check.isin([0, 1]), nullable=False),

        # b. Numeric columns with missing values allowed up to 5%
        "st_depression": Column(
            float,

            nullable=True
        ),
        "num_of_vessels": Column(
            float,
            checks=[
                Check(lambda s: np.isnan(s) | (
                    (s >= 0) & (s <= 4)), element_wise=True),
                Check(lambda s: np.isnan(s).mean() <= 0.05, element_wise=False,
                      error="Too many null values in 'num_of_vessels' column.")
            ],
            nullable=True
        ),

        # c. Categorical column with missing values allowed up to 5%
        "thalassemia": Column(
            str,
            checks=[
                Check.isin(["normal", "fixed defect", "reversable defect"]),
                Check(lambda s: s.isna().mean() <= 0.05, element_wise=False,
                      error="Too many null values in 'thalassemia' column."),
            ],
            nullable=True
        )
    }
)

# Validate Schema
try:
    missingness_threshold_schema.validate(df)
    # print("Validation passed: No missingness beyond expected threshold.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 2 & 5. Check for correct column names and correct data types in each column

# Build Schema
column_type_schema = pa.DataFrameSchema(
    {
        "age": pa.Column(pa.Int),
        "sex": pa.Column(pa.Int),
        "chest_pain_type": pa.Column(pa.String),
        "resting_blood_pressure": pa.Column(pa.Int),
        "cholesterol": pa.Column(pa.Int),
        "fasting_blood_sugar": pa.Column(pa.Int),
        "rest_ecg": pa.Column(pa.String),
        "max_heart_rate": pa.Column(pa.Int),
        "exercise_induced_angina": pa.Column(pa.String),
        "st_depression": pa.Column(pa.Float),
        "slope": pa.Column(pa.String),
        "num_of_vessels": pa.Column(pa.Float),
        "thalassemia": pa.Column(pa.String),
        "diagnosis": pa.Column(pa.Int)
    }
)

# Validate Schema
try:
    column_type_schema.validate(df)
    # print("Validation passed: All columns have correct data types.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 6. No duplicate observations

# Build Schema
duplicate_schema = pa.DataFrameSchema(
    checks=[
        Check(lambda df: ~df.duplicated().any(), error="Duplicate rows found.")
    ]
)

# Validate Schema
try:
    duplicate_schema.validate(df)
    # print("Validation passed: No duplicates found.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 7. No Outlier or Anamalous Values

# Build Schema
numerical_schema = pa.DataFrameSchema(
    {
        "age": Column(int, Check.between(0, 120), nullable=True),
        "resting_blood_pressure": Column(int, Check.between(70, 200), nullable=True),
        "cholesterol": Column(int, Check.between(100, 600), nullable=True),
        "max_heart_rate": Column(int, Check.between(60, 220), nullable=True),
        "fasting_blood_sugar": pa.Column(int, pa.Check.isin([0, 1]), nullable=False)  
    }
)

# Validate Schema
try:
    numerical_schema.validate(df)
    # print("Validation passed: No outliers found.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 8: Correct Category Levels

# schema for mapped categorical values

map_categorical_schema = pa.DataFrameSchema(
    {
        "chest_pain_type": Column(
            str, Check.isin(["typical angina", "atypical angina",
                            "non-anginal pain", "asymptomatic"])
        ),
        "fasting_blood_sugar": Column(
            int, Check.isin([0, 1])
        ),
        "rest_ecg": Column(
            str, Check.isin(["normal", "ST-T wave abnormality",
                            "left ventricular hypertrophy"])
        ),
        "exercise_induced_angina": Column(
            str, Check.isin(["no", "yes"])
        ),
        "slope": Column(
            str, Check.isin(["upsloping", "flat", "downsloping"])
        ),
        "thalassemia": Column(
            str, Check.isin(["normal", "fixed defect", "reversable defect"])
        ),
        "diagnosis": Column(
            int, Check.isin([0, 1])
        )
    }
)

# checking mapped categorical values
try:
    map_categorical_schema.validate(df)
    # print("Validation passed: All categorical mappings are correct.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 9. Check Target/Response Variable Data Distribution

def check_proportions(series, tolerance=0.1):
    """
    Checks if the proportions of class labels (0 and 1) in a given pandas Series 
    are approximately balanced within a specified tolerance.

    Parameters:
    series (pandas.Series): A pandas Series containing binary class labels (0s and 1s).
    tolerance (float, optional): The acceptable tolerance for the proportions of each class.
                                 The default is 0.1, meaning the proportions of 0 and 1 
                                 must be within 0.1 of 0.5 to return True.

    Returns:
    bool: True if the proportions of class 0 and class 1 are approximately equal 
          (within the specified tolerance), False otherwise.
    """

    # Calculate the normalized proportions of the unique values
    proportions = series.value_counts(normalize=True)

    # Print the calculated proportions
    # print("class proportions are", proportions)

    # Check if the proportions of 0 and 1 are within the acceptable tolerance of 0.5
    return np.abs(proportions.get(0, 0) - 0.5) <= tolerance and np.abs(proportions.get(1, 0) - 0.5) <= tolerance


# Define the schema for validating the balance
proportion_check_schema = pa.DataFrameSchema(
    {
        "diagnosis": Column(
            int,
            Check(lambda s: check_proportions(s),
                  error="Class proportions are not balanced.")
        )
    }
)

# Try to validate the dataframe using the schema
try:
    proportion_check_schema.validate(df)
    # print("Validation passed: Class proportions are as expected.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 10. Deep check anomalous correlations between target/response variable and features/explanatory variables

deepchecks_dataset = Dataset(
    df,
    label="diagnosis",
    cat_features=[]
)

check_feat_lab_corr = FeatureLabelCorrelation()

check_feat_lab_corr = (check_feat_lab_corr
                       .add_condition_feature_pps_less_than(0.9))

result = check_feat_lab_corr.run(dataset=deepchecks_dataset)

# Print the result of the check
# print(result)

if not result.passed_conditions():
    raise ValueError(
        "Feature-Label correlation exceeds the acceptable threshold."
    )
```

```{python}
# 11. Deep check anomalous correlations between features/explanatory variables

deepchecks_dataset = Dataset(
    df,
    label="diagnosis",
    cat_features=[]
)

check_feat_feat_corr = FeatureFeatureCorrelation(threshold=0.9)
result = check_feat_feat_corr.run(dataset=deepchecks_dataset)

# Print the result of the check
# print(result)

if not result.passed_conditions():
    raise ValueError(
        "Anomalous correlations between features found."
        "Some feature correlations exceed the acceptable threshold."
    )
```

```{python}
# Export cleaned data
df.to_csv("../data/processed/processed_heart_disease_data.csv", index=False)
```

For the data validation we ensured:


- No empty observations were found.
- No missingness was beyond the expected threshold.
- All columns have correct data types.
- No duplicates were found
- No outliers were found.
- All categorical mappings were correct.
- No class imbalance
