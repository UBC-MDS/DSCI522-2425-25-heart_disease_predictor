# Data Processing

```{python}
import numpy as np
import pandas as pd
import warnings
import pandera as pa
from pandera import Column, Check, DataFrameSchema


from deepchecks.tabular import Dataset
from deepchecks.tabular.checks import FeatureLabelCorrelation, FeatureFeatureCorrelation, PredictionDrift

warnings.filterwarnings('ignore')
```

```{python}
# Retrieve the data from the ucimlrepo
from ucimlrepo import fetch_ucirepo 

heart_disease = fetch_ucirepo(id=45)

X = heart_disease.data.features  
y = heart_disease.data.targets   

data = X.copy()
data['Diagnosis'] = y  

print("Dataset Shape:", data.shape)
print("Columns:", data.columns)

# Save the raw 

data.to_csv("../data/raw/raw_heart_disease_data.csv", index=False)
```

```{python}
df = pd.read_csv("../data/raw/raw_heart_disease_data.csv")
df
```

```{python}
# Rename column names to be human readable

new_column_names = ["age", "sex", "chest_pain_type", "resting_blood_pressure", "cholesterol", "fasting_blood_sugar", 
                "rest_ecg", "max_heart_rate", "exercise_induced_angina", "st_depression", "slope", 
               "num_of_vessels", "thalassemia", "diagnosis"]


df.columns = new_column_names
```

## Checking the unique values for each column

```{python}
df['num_of_vessels'].unique()
```

```{python}
df['sex'].unique()
```

```{python}
df['chest_pain_type'].unique()
```

```{python}
df['fasting_blood_sugar'].unique()
```

```{python}
df['rest_ecg'].unique()
```

```{python}
df['thalassemia'].unique()
```

## Map the values with the provided labels

```{python}
df.loc[(df['chest_pain_type'] == 1), 'chest_pain_type'] = 'typical angina'
df.loc[(df['chest_pain_type'] == 2), 'chest_pain_type'] = 'atypical angina'
df.loc[(df['chest_pain_type'] == 3), 'chest_pain_type'] = 'non-anginal pain'
df.loc[(df['chest_pain_type'] == 4), 'chest_pain_type'] = 'asymptomatic'
```

```{python}
df.loc[(df['fasting_blood_sugar'] == 'yes'), 'fasting_blood_sugar'] = 1
df.loc[(df['fasting_blood_sugar'] == 'no'), 'fasting_blood_sugar'] = 0
```

```{python}
df.loc[(df['rest_ecg'] == 0), 'rest_ecg'] = 'normal'
df.loc[(df['rest_ecg'] == 1), 'rest_ecg'] = 'ST-T wave abnormality'
df.loc[(df['rest_ecg'] == 2), 'rest_ecg'] = 'left ventricular hypertrophy'
```

```{python}
df.loc[(df['exercise_induced_angina'] == 0), 'exercise_induced_angina'] = 'no'
df.loc[(df['exercise_induced_angina'] == 1), 'exercise_induced_angina'] = 'yes'
```

```{python}
df.loc[(df['slope'] == 1), 'slope'] = 'upsloping'
df.loc[(df['slope'] == 2), 'slope'] = 'flat'
df.loc[(df['slope'] == 3), 'slope'] = 'downsloping'
```

```{python}
df.loc[(df['thalassemia'] == 3.), 'thalassemia'] = 'normal'
df.loc[(df['thalassemia'] == 6.), 'thalassemia'] = 'fixed defect'
df.loc[(df['thalassemia'] == 7.), 'thalassemia'] = 'reversable defect'
```

```{python}
df.loc[(df['diagnosis'] == 2), 'diagnosis'] = 1
df.loc[(df['diagnosis'] == 3), 'diagnosis'] = 1
df.loc[(df['diagnosis'] == 4), 'diagnosis'] = 1
```

```{python}
df
```

## Checking and removing null values

```{python}
df.isnull().sum()
```

```{python}
df = df.dropna()
```

```{python}
df.isnull().sum()
```

## Data Validation

Since we have imported data from the ucimlrepo, we will not be checking for correct data file format. 

```{python}
# 3. Check for empty observations

# Build Schema
empty_obs_schema = pa.DataFrameSchema(
    checks=[
        Check(lambda df: ~(df.isna().all(axis=1)).any(), error="Empty rows found.")
    ]
)

# Validate Schema
try:
    empty_obs_schema.validate(df)
    print("Validation passed: No empty observations found.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 4. Missingness not beyond expected threshold

# Build Schema
missingness_threshold_schema = pa.DataFrameSchema(
    {
        # a. Numeric columns with no missing values allowed
        "age": pa.Column(int, pa.Check.between(0, 120), nullable=False),
        "sex": pa.Column(int, pa.Check.isin([0, 1]), nullable=False),

        # b. Numeric columns with missing values allowed up to 5%
        "st_depression": Column(
            float,

            nullable=True
        ),
        "num_of_vessels": Column(
            float,
            checks=[
                Check(lambda s: np.isnan(s) | (
                    (s >= 0) & (s <= 4)), element_wise=True),
                Check(lambda s: np.isnan(s).mean() <= 0.05, element_wise=False,
                      error="Too many null values in 'num_of_vessels' column.")
            ],
            nullable=True
        ),

        # c. Categorical column with missing values allowed up to 5%
        "thalassemia": Column(
            str,
            checks=[
                Check.isin(["normal", "fixed defect", "reversable defect"]),
                Check(lambda s: s.isna().mean() <= 0.05, element_wise=False,
                      error="Too many null values in 'thalassemia' column."),
            ],
            nullable=True
        )
    }
)

# Validate Schema
try:
    missingness_threshold_schema.validate(df)
    print("Validation passed: No missingness beyond expected threshold.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 2 & 5. Check for correct column names and correct data types in each column

# Build Schema
column_type_schema = pa.DataFrameSchema(
    {
        "age": pa.Column(pa.Int),
        "sex": pa.Column(pa.Int),
        "chest_pain_type": pa.Column(pa.String),
        "resting_blood_pressure": pa.Column(pa.Int),
        "cholesterol": pa.Column(pa.Int),
        "fasting_blood_sugar": pa.Column(pa.Int),
        "rest_ecg": pa.Column(pa.String),
        "max_heart_rate": pa.Column(pa.Int),
        "exercise_induced_angina": pa.Column(pa.String),
        "st_depression": pa.Column(pa.Float),
        "slope": pa.Column(pa.String),
        "num_of_vessels": pa.Column(pa.Float),
        "thalassemia": pa.Column(pa.String),
        "diagnosis": pa.Column(pa.Int)
    }
)

# Validate Schema
try:
    column_type_schema.validate(df)
    print("Validation passed: All columns have correct data types.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 6. No duplicate observations

# Build Schema
duplicate_schema = pa.DataFrameSchema(
    checks=[
        Check(lambda df: ~df.duplicated().any(), error="Duplicate rows found.")
    ]
)

# Validate Schema
try:
    duplicate_schema.validate(df)
    print("Validation passed: No duplicates found.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 7. No Outlier or Anamalous Values

# Build Schema
numerical_schema = pa.DataFrameSchema(
    {
        "age": Column(int, Check.between(0, 120), nullable=True),
        "resting_blood_pressure": Column(int, Check.between(70, 200), nullable=True),
        "cholesterol": Column(int, Check.between(100, 600), nullable=True),
        "max_heart_rate": Column(int, Check.between(60, 220), nullable=True),
        "fasting_blood_sugar": pa.Column(int, pa.Check.isin([0, 1]), nullable=False)  
    }
)

# Validate Schema
try:
    numerical_schema.validate(df)
    print("Validation passed: No outliers found.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 8: Correct Category Levels

# schema for mapped categorical values

map_categorical_schema = pa.DataFrameSchema(
    {
        "chest_pain_type": Column(
            str, Check.isin(["typical angina", "atypical angina",
                            "non-anginal pain", "asymptomatic"])
        ),
        "fasting_blood_sugar": Column(
            int, Check.isin([0, 1])
        ),
        "rest_ecg": Column(
            str, Check.isin(["normal", "ST-T wave abnormality",
                            "left ventricular hypertrophy"])
        ),
        "exercise_induced_angina": Column(
            str, Check.isin(["no", "yes"])
        ),
        "slope": Column(
            str, Check.isin(["upsloping", "flat", "downsloping"])
        ),
        "thalassemia": Column(
            str, Check.isin(["normal", "fixed defect", "reversable defect"])
        ),
        "diagnosis": Column(
            int, Check.isin([0, 1])
        )
    }
)

# checking mapped categorical values
try:
    map_categorical_schema.validate(df)
    print("Validation passed: All categorical mappings are correct.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 9. Check Target/Response Variable Data Distribution

def check_proportions(series, tolerance=0.1):
    """
    Checks if the proportions of class labels (0 and 1) in a given pandas Series 
    are approximately balanced within a specified tolerance.

    Parameters:
    series (pandas.Series): A pandas Series containing binary class labels (0s and 1s).
    tolerance (float, optional): The acceptable tolerance for the proportions of each class.
                                 The default is 0.1, meaning the proportions of 0 and 1 
                                 must be within 0.1 of 0.5 to return True.

    Returns:
    bool: True if the proportions of class 0 and class 1 are approximately equal 
          (within the specified tolerance), False otherwise.
    """

    # Calculate the normalized proportions of the unique values
    proportions = series.value_counts(normalize=True)

    # Print the calculated proportions
    print("class proportions are", proportions)

    # Check if the proportions of 0 and 1 are within the acceptable tolerance of 0.5
    return np.abs(proportions.get(0, 0) - 0.5) <= tolerance and np.abs(proportions.get(1, 0) - 0.5) <= tolerance


# Define the schema for validating the balance
proportion_check_schema = pa.DataFrameSchema(
    {
        "diagnosis": Column(
            int,
            Check(lambda s: check_proportions(s),
                  error="Class proportions are not balanced.")
        )
    }
)

# Try to validate the dataframe using the schema
try:
    proportion_check_schema.validate(df)
    print("Validation passed: Class proportions are as expected.")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

```{python}
# 10. Deep check anomalous correlations between target/response variable and features/explanatory variables

deepchecks_dataset = Dataset(
    df,
    label="diagnosis",
    cat_features=[]
)

check_feat_lab_corr = FeatureLabelCorrelation()

check_feat_lab_corr = (check_feat_lab_corr
                       .add_condition_feature_pps_less_than(0.9))

result = check_feat_lab_corr.run(dataset=deepchecks_dataset)

# Print the result of the check
print(result)

if not result.passed_conditions():
    raise ValueError(
        "Feature-Label correlation exceeds the acceptable threshold."
    )
```

```{python}
# 11. Deep check anomalous correlations between features/explanatory variables

deepchecks_dataset = Dataset(
    df,
    label="diagnosis",
    cat_features=[]
)

check_feat_feat_corr = FeatureFeatureCorrelation(threshold=0.9)
result = check_feat_feat_corr.run(dataset=deepchecks_dataset)

# Print the result of the check
print(result)

if not result.passed_conditions():
    raise ValueError(
        "Anomalous correlations between features found."
        "Some feature correlations exceed the acceptable threshold."
    )
```

```{python}
# Export cleaned data
df.to_csv("../data/processed/processed_heart_disease_data.csv", index=False)
```
